{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation modules--\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Silencing the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualization packages--\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import missingno as msno \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation - Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are one of the most common problems one encounters during data preparation. They affect the performance of the machine learning models to a greater extent.\n",
    "\n",
    "For the following exercises, the volcano dataset has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano = pd.read_csv(\"volcano_data.csv\")\n",
    "volcano.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"Data Dictionary.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of nulls in the dataset\n",
    "print(\"There are {0} missing values in the dataset.\".format(volcano.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values as a matrix \n",
    "msno.matrix(volcano) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most simple solution to the missing values is to drop the rows or the entire column. There is no set threshold for dropping rows/columns and no method to mathematically determine the optimum threshold. In this case 70% is used as a threshold, i.e. rows/columns which have missing values more than this threshold are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the threshold to 70%\n",
    "threshold = 0.7\n",
    "data = volcano\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]\n",
    "\n",
    "# Before treating the missing values\n",
    "print(\"Before dropping the records with missing values - {0}.\".format(volcano.shape))\n",
    "print(\"After dropping the records - {0}.\".format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Imputation\n",
    "**Imputation is preferable compared to dropping rows/columns as it preserves data size**. In this case, it is important to select the imputation value accurately. It is generally desirable to have a default value which replaces missing values. This is especially easy for binary variables, for example, if a column has 1 and NA, then it is likely that the NA rows correspond to 0. However, the same cannot be necessarily performed for variables on a ratio scale. For example, the missing values of the variable “number of customer visits in the previous month”, should be replaced with 0 only if one were sure that no customers could have visited that outlet in the last month\n",
    "\n",
    "One of the reasons for the presence of missing values is when tables with different sizes are joined together, in which case imputing with 0 is reasonable.\n",
    "\n",
    "Except when a default imputation value exists, the best imputation technique is using the medians of respective column. The median medians is preferred as it is the least sensitive to outliers compared to other measures of central tendency, and thus is the most representative of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering for numerical columns\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numericaldf = volcano.select_dtypes(include=numerics)\n",
    "\n",
    "print(\"Here's is the list of numerical variables - {0}\".format(numericaldf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values before imputation -\\n{0}\".format(numericaldf.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values as a matrix \n",
    "msno.matrix(numericaldf) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling missing values with medians of the columns\n",
    "data = numericaldf\n",
    "data = data.fillna(data.median())\n",
    "\n",
    "print(\"Filling missing values with medians of the columns\\n\")\n",
    "\n",
    "print(\"Missing values after imputating median values -\\n{0}\".format(data.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values as a matrix \n",
    "msno.matrix(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Imputation\n",
    "\n",
    "Replacing the missing values with the mode (maximum occurred value) of a column is desirable when handling categorical columns. But if the values in the column are distributed uniformly and there no dominant value, imputing a category like “Others” might be more sensible, because in such cases, the imputation is likely to converge to a random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricaldf = volcano.select_dtypes(exclude=numerics)\n",
    "\n",
    "print(\"Here's is the list of categorical variables - \\n{0}\".format(categoricaldf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Missing values before imputation -\\n{0}\".format(categoricaldf.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values as a matrix \n",
    "msno.matrix(categoricaldf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column Agent column has 40 null values. Here the mode (maximum occurring) value is imputed in place of all missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "data = categoricaldf\n",
    "data['Agent'].fillna(data['Agent'].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values in the columns **TSU** and **EQ** are where the volcano didn't cause a Tsunami or an earthquake. So replacing them with mode will cause errors in the data. Hence replacing the values with **'No TSU'** and **'No EQ'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TSU'].fillna(\"No TSU\", inplace=True)\n",
    "data['EQ'].fillna(\"No EQ\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values after imputation -\\n{0}\".format(data.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values as a matrix \n",
    "msno.matrix(data) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, TSU and EQ have no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An outlier is a data point that differs significantly from other observations. An outlier occurs due to variability in measurement or due to an experimental error, with the latter mostly being excluded from the data. An outlier can cause serious problems in statistical analyses. There are two different ways of handling outliers – using standard deviation, and percentiles. Here we are using the iris flowers dataset understanding the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.boxplot(data=iris, orient=\"h\", palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection with Standard Deviation\n",
    "If a value is located at a distance greater than n standard deviations, then it is considered as an outlier. How is n determined? While there is no definite mathematical solution to determine n, a value between 2 and 4 usually provides the best results in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "data = sns.load_dataset(\"iris\")\n",
    "upper_lim = data['sepal_width'].mean () + data['sepal_width'].std () * factor\n",
    "lower_lim = data['sepal_width'].mean () - data['sepal_width'].std () * factor\n",
    "\n",
    "data = data[(data['sepal_width'] < upper_lim) & (data['sepal_width'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.boxplot(data = data, orient=\"h\", palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection with Percentiles\n",
    "Another mathematical method to detect outliers is to use percentiles. The Inter Quartile Range (IQR) rule is useful in detecting the presence of outliers. In general, any number which lies above or below 1.5IQR is considered as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with Percentiles\n",
    "data = sns.load_dataset(\"iris\")\n",
    "q3 = data['sepal_width'].quantile(.75)\n",
    "q1 = data['sepal_width'].quantile(.25)\n",
    "IQR = q3 - q1\n",
    "\n",
    "upper_lim = q3 + (1.5 * IQR)\n",
    "lower_lim = q1 - (1.5 * IQR)\n",
    "\n",
    "\n",
    "data = data[(data['sepal_width'] < upper_lim) & (data['sepal_width'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.boxplot(data = data, orient=\"h\", palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Outlier Dilemma: Drop or Cap\n",
    "Another option for handling outliers is to cap them instead of dropping so that the data size remains same and hence the model can be trained well. On the other hand, capping can affect the distribution of the data, thus it is better not to treat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "data = iris\n",
    "q3 = data['sepal_width'].quantile(.75)\n",
    "q1 = data['sepal_width'].quantile(.25)\n",
    "IQR = q3 - q1\n",
    "\n",
    "upper_lim = q3 + (1.5 * IQR)\n",
    "lower_lim = q1 - (1.5 * IQR)\n",
    "\n",
    "data.loc[(data['sepal_width'] > upper_lim),'sepal_width'] = upper_lim\n",
    "data.loc[(data['sepal_width'] < lower_lim),'sepal_width']= lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.boxplot(data = data, orient=\"h\", palette=\"Set1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
